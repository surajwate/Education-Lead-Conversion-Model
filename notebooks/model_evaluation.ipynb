{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import joblib\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "notebooks_path = os.getcwd()\n",
    "notebooks_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(Path().resolve().parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(str(Path().resolve().parent))\n",
    "\n",
    "from src.create_folds import create_folds\n",
    "from src.data_cleaning import drop_columns, impute_missing_values\n",
    "from src.data_preprocessing import preprocess_data\n",
    "from src.log_reg_model import train_logistic_regression\n",
    "from src.model_utils import evaluate_model, load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Data Cleaning\n",
    "data = load_data(Path(\"../input/train_folds.csv\"))\n",
    "df = drop_columns(data)\n",
    "\n",
    "all_accuracies = []\n",
    "\n",
    "for fold in range(5):\n",
    "    logging.info(f\"Processing fold {fold}...\")\n",
    "    \n",
    "    # Step 3: Data Imputation\n",
    "    fold_df = impute_missing_values(df, fold)\n",
    "    \n",
    "    # Step 4: Data Preprocessing\n",
    "    fold_df = preprocess_data(fold_df, fold)\n",
    "    \n",
    "    # Step 5: Model Training\n",
    "    X = fold_df.drop([\"Converted\", \"kfold\"], axis=1)\n",
    "    y = fold_df[\"Converted\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = train_logistic_regression(X_train, y_train)\n",
    "    \n",
    "    # Step 6: Model Evaluation\n",
    "    accuracy, report, matrix = evaluate_model(model, X_test, y_test)\n",
    "    all_accuracies.append(accuracy)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Fold {fold} Results:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    # print(f\"Classification Report:\\n{report}\")\n",
    "    # print(f\"Confusion Matrix:\\n{matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0\n",
    "logging.info(f\"Processing fold {fold}...\")\n",
    "\n",
    "# Step 3: Data Imputation\n",
    "fold_df = impute_missing_values(df, fold)\n",
    "\n",
    "# Step 4: Data Preprocessing\n",
    "fold_df = preprocess_data(fold_df, fold)\n",
    "\n",
    "# Step 5: Model Training\n",
    "X = fold_df.drop([\"Converted\", \"kfold\"], axis=1)\n",
    "y = fold_df[\"Converted\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = train_logistic_regression(X_train, y_train)\n",
    "\n",
    "# Step 6: Model Evaluation\n",
    "accuracy, report, matrix = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "# Print results\n",
    "print(f\"Fold {fold} Results:\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "# print(f\"Classification Report:\\n{report}\")\n",
    "# print(f\"Confusion Matrix:\\n{matrix}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance\n",
    "\n",
    "coefficients = model.coef_[0]\n",
    "# odds_ratios = [round(float(x), 2) for x in list(map(lambda x: 2**x, coefficients))]\n",
    "odds_ratios = np.exp(coefficients)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': coefficients,\n",
    "    'Odds Ratio': odds_ratios\n",
    "})\n",
    "\n",
    "# Sort by absolute value of coefficient\n",
    "feature_importance = feature_importance.reindex(feature_importance['Coefficient'].abs().sort_values(ascending=False).index)\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probability of conversion\n",
    "probs = model.predict_proba(X_test)[:, 1] # Probability of the positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a custom threshold to get the predicted class\n",
    "threshold = 0.45\n",
    "y_pred = (probs >= threshold).astype(int)\n",
    "\n",
    "# Evaluate the model with the custom threshold\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy with threshold {threshold}: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the ROC Curve and Calculating AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
    "\n",
    "# Calculate AUC\n",
    "auc_score = roc_auc_score(y_test, probs)\n",
    "print(f\"AUC: {auc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {auc_score:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Optimal Cut-off Point\n",
    "\n",
    "Optimal cut-off point is the point where we get the best accuracy. We can find it by calculating the point where the sum of sensitivity and specificity is maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create columns with different probability thresholds\n",
    "thresholds = np.linspace(0, 0.99, 100)\n",
    "metrics = []\n",
    "best_threshold = 0.5\n",
    "best_f1, best_accuracy, best_precision, best_recall = 0, 0, 0, 0\n",
    "best_f1_threshold, best_accuracy_threshold, best_precision_threshold, best_recall_threshold = 0, 0, 0, 0\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred = (probs >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    metrics.append([threshold, accuracy, precision, recall, f1])\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_f1_threshold = threshold\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_accuracy_threshold = threshold\n",
    "    if precision > best_precision:\n",
    "        best_precision = precision\n",
    "        best_precision_threshold = threshold\n",
    "    if recall > best_recall:\n",
    "        best_recall = recall\n",
    "        best_recall_threshold = threshold\n",
    "    if recall == precision:\n",
    "        best_threshold = threshold\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics, columns=[\"Threshold\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "metrics_df = metrics_df.set_index(\"Threshold\")\n",
    "\n",
    "print(f\"Best F1-Score threshold: {best_f1_threshold}, Best F1-Score: {best_f1}\")\n",
    "print(f\"Best Accuracy threshold: {best_accuracy_threshold}, Best Accuracy: {best_accuracy}\")\n",
    "print(f\"Best Precision threshold: {best_precision_threshold}, Best Precision: {best_precision}\")\n",
    "print(f\"Best Recall threshold: {best_recall_threshold}, Best Recall: {best_recall}\")\n",
    "print(f\"Best threshold: {best_threshold}\")\n",
    "\n",
    "# Plot the metrics\n",
    "plt.figure(figsize=(8, 6))\n",
    "metrics_df.plot()\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Metrics vs. Threshold\")\n",
    "plt.legend(title=\"Metric\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Youden's Index\n",
    "\n",
    "Youden's Index ia a statistic that is often used to evaluate the effectiveness of a diagnostic test or, in the context of machine learning, to assess the performance of a binary classification model. It helps to identify the optimal threshold that maximized the balance between the true positive rate (sensitivity) and the false positive rate (1-specificity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold(y_true, y_prob):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "    youdens_index = tpr - fpr\n",
    "    optimal_threshold = thresholds[np.argmax(youdens_index)]\n",
    "    return optimal_threshold\n",
    "\n",
    "youden_threshold = find_optimal_threshold(y_test, probs)\n",
    "print(f\"Optimal threshold: {youden_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Calculate precision and recall for different thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, probs)\n",
    "\n",
    "# Step 3: Calculate the average precision score\n",
    "average_precision = average_precision_score(y_test, probs)\n",
    "print(f'Average Precision Score: {average_precision:.2f}')\n",
    "\n",
    "# Step 4: Plot the Precision-Recall curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, marker='.', label=f'Logistic Regression (AP = {average_precision:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_thresholds(probs, y_test, thresholds=np.linspace(0, 0.99, 1000), tol=0.01):\n",
    "    metrics = []\n",
    "    best_f1, best_accuracy, best_precision, best_recall = 0, 0, 0, 0\n",
    "    best_f1_threshold, best_accuracy_threshold, best_precision_threshold, best_recall_threshold = 0, 0, 0, 0\n",
    "    best_threshold = 0.1\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (probs >= threshold).astype(int)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        metrics.append([threshold, accuracy, precision, recall, f1])\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_f1_threshold = threshold\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_accuracy_threshold = threshold\n",
    "        if precision > best_precision:\n",
    "            best_precision = precision\n",
    "            best_precision_threshold = threshold\n",
    "        if recall > best_recall:\n",
    "            best_recall = recall\n",
    "            best_recall_threshold = threshold\n",
    "        if abs(recall - precision) <= tol:  # Compare within the tolerance\n",
    "            best_threshold = threshold\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics, columns=[\"Threshold\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "    metrics_df = metrics_df.set_index(\"Threshold\")\n",
    "\n",
    "    print(f\"Best F1-Score threshold: {best_f1_threshold}, Best F1-Score: {best_f1}\")\n",
    "    print(f\"Best Accuracy threshold: {best_accuracy_threshold}, Best Accuracy: {best_accuracy}\")\n",
    "    print(f\"Best Precision threshold: {best_precision_threshold}, Best Precision: {best_precision}\")\n",
    "    print(f\"Best Recall threshold: {best_recall_threshold}, Best Recall: {best_recall}\")\n",
    "    print(f\"Best threshold (Recall â‰ˆ Precision): {best_threshold}\")\n",
    "\n",
    "    return metrics_df, best_f1_threshold, best_accuracy_threshold, best_precision_threshold, best_recall_threshold, best_threshold\n",
    "\n",
    "\n",
    "def plot_metrics(metrics_df):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    metrics_df.plot()\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Metrics vs. Threshold\")\n",
    "    plt.legend(title=\"Metric\")\n",
    "    plt.grid(True, which='both')\n",
    "    plt.minorticks_on()\n",
    "    plt.grid(True, which='minor', linestyle=':', linewidth='0.5', color='gray', alpha=0.3)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "probs = model.predict_proba(X_test)[:, 1]\n",
    "metrics_df, best_f1_threshold, best_accuracy_threshold, best_precision_threshold, best_recall_threshold, best_threshold = evaluate_thresholds(probs, y_test)\n",
    "plot_metrics(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_metrics(metrics_df, figsize=(15, 10), title=\"Metrics vs. Threshold\", \n",
    "                 custom_colors=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot metrics against thresholds with improved formatting and options.\n",
    "    \n",
    "    Parameters:\n",
    "    - metrics_df: DataFrame with metrics (columns) and thresholds (index)\n",
    "    - figsize: Tuple for figure size\n",
    "    - title: String for plot title\n",
    "    - custom_colors: Dict mapping metric names to colors (optional)\n",
    "    - save_path: String path to save the figure (optional)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Use custom colors if provided, otherwise use default color cycle\n",
    "    if custom_colors:\n",
    "        for column, color in custom_colors.items():\n",
    "            plt.plot(metrics_df.index, metrics_df[column], label=column, color=color, linewidth=2)\n",
    "    else:\n",
    "        for column in metrics_df.columns:\n",
    "            plt.plot(metrics_df.index, metrics_df[column], label=column, linewidth=2)\n",
    "    \n",
    "    plt.xlabel(\"Threshold\", fontsize=12)\n",
    "    plt.ylabel(\"Score\", fontsize=12)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.legend(title=\"Metric\", title_fontsize=12, fontsize=10, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    \n",
    "    # Set x-axis ticks\n",
    "    plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "    \n",
    "    # Set y-axis ticks\n",
    "    plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "    \n",
    "    # Add grid\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.minorticks_on()\n",
    "    plt.grid(which='minor', linestyle=':', linewidth='0.5', color='gray', alpha=0.3)\n",
    "    \n",
    "    # Add horizontal lines at 0.25, 0.5, and 0.75 for easier reading\n",
    "    for y in [0.25, 0.5, 0.75]:\n",
    "        plt.axhline(y=y, color='gray', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    # Improve layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Add text annotations for max values\n",
    "    for column in metrics_df.columns:\n",
    "        max_value = metrics_df[column].max()\n",
    "        max_threshold = metrics_df[column].idxmax()\n",
    "        plt.annotate(f'Max {column}: {max_value:.2f} at {max_threshold:.2f}',\n",
    "                     xy=(max_threshold, max_value), xytext=(5, 5),\n",
    "                     textcoords='offset points', ha='left', va='bottom',\n",
    "                     bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "                     arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "    \n",
    "    # Save the figure if a path is provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "custom_colors = {'Accuracy': 'blue', 'Precision': 'red', 'Recall': 'green', 'F1': 'purple'}\n",
    "plot_metrics(metrics_df, custom_colors=custom_colors)\n",
    "# plot_metrics(metrics_df, custom_colors=custom_colors, save_path='metrics_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics based on the best threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the chosen threshold\n",
    "chosen_threshold = 0.46\n",
    "\n",
    "# Apply the threshold to get predictions\n",
    "y_pred = (probs >= chosen_threshold).astype(int)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc_score = roc_auc_score(y_test, probs)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Metrics at Threshold {chosen_threshold}:\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "print(f\"AUC: {auc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0.30, 0.64, 17)\n",
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of thresholds to evaluate\n",
    "thresholds = np.linspace(0.30, 0.64, 34)\n",
    "\n",
    "for chosen_threshold in thresholds:\n",
    "    # Apply the threshold to get predictions\n",
    "    y_pred = (probs >= chosen_threshold).astype(int)\n",
    "\n",
    "    # Calculate the metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc_score = roc_auc_score(y_test, probs)\n",
    "\n",
    "    print(f\"Threshold {chosen_threshold:.2f}: Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}, AUC: {auc_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the business requirement, where CEO has given a ballpark of the target lead conversion rate to be around 80%, we need precision atleast 80% to consider the model to be good. \n",
    "\n",
    "At the threshold of 0.62, we start getting precision of 80%. So, we will consider this threshold as the best threshold for our model.\n",
    "It's because after this threshold, even though the precision is increasing, the recall and F1 score are decreasing. So, we will consider the threshold of 0.62 as the best threshold for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics of all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0\n",
    "\n",
    "logging.info(f\"Processing fold {fold}...\")\n",
    "\n",
    "# Step 3: Data Imputation\n",
    "fold_df = impute_missing_values(df, fold)\n",
    "\n",
    "# Step 4: Data Preprocessing\n",
    "fold_df = preprocess_data(fold_df, fold)\n",
    "\n",
    "# Step 5: Model Training\n",
    "X = fold_df.drop([\"Converted\", \"kfold\"], axis=1)\n",
    "y = fold_df[\"Converted\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = train_logistic_regression(X_train, y_train)\n",
    "# Predict probabilities\n",
    "probs = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model with the custom threshold\n",
    "threshold = 0.62\n",
    "y_pred = (probs >= threshold).astype(int)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc_score = roc_auc_score(y_test, probs)\n",
    "\n",
    "# Print results\n",
    "print(f\"Fold {fold} Results: Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}, AUC: {auc_score:.2f}\")\n",
    "print()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance for fold 0\n",
    "coefficients = model.coef_[0]\n",
    "odds_ratios = np.exp(coefficients)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': coefficients,\n",
    "    'Odds Ratio': odds_ratios\n",
    "})\n",
    "\n",
    "# Sort by absolute value of coefficient\n",
    "feature_importance = feature_importance.reindex(feature_importance['Coefficient'].abs().sort_values(ascending=False).index)\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance\n",
    "feature_importance = pd.Series(model.coef_[0], index=X_train.columns)\n",
    "feature_importance = feature_importance.sort_values(ascending=False)\n",
    "print(f\"Feature importance:\\n{feature_importance}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
